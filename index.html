
<!DOCTYPE HTML "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.text-justify {
    text-align: justify;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: rgb(73, 73, 73);
    text-align: center;
    margin-top: 15px;
    margin-bottom: 12px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.insight {
  background-color: #EEEEEE;
  padding-right: 100px;
  padding-left: 100px;
  padding-top: 20px;
  padding-bottom: 20px;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 18px;
  width: 90px;
  font-weight: 400;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}
.controls {
  margin-bottom: 10px;
  margin-top: 20px;
}
.left-controls {
  display: inline-block;
  vertical-align: top;
  width: 80%;
}
.right-controls {
  display: inline-block;
  vertical-align: top;
  width: 19%;
  text-align: right;
}

.render_window {
    display: inline-block;
    vertical-align: middle;
    box-shadow: 0px 0px 0px black;
    margin-right: 20px;
    margin-bottom: 20px;
    width: calc(33% - 20px);
}

.center {
    display: block;
    margin-left: auto;
    margin-right: auto;
}

</style>


<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://www.nvidia.com/en-us/research/"><strong>NVIDIA Research</strong></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>Optimizing Data Collection for Machine Learning</title>
    <meta property="og:description" content="Optimizing Data Collection for Machine Learning"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<!--
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>
-->

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>Optimizing Data Collection For Machine Learning</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="https://rafidrm.github.io/">Rafid Mahmood</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.toronto.edu/~jlucas/">James Lucas</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="http://www.cs.toronto.edu/~law/">Marc T. Law</a><sup>1</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-3 text-center"><sup>1</sup>NVIDIA</a></div>
            <div class="col-3 text-center"><sup>2</sup>University of Toronto</div>
            <div class="col-3 text-center"><sup>3</sup>Vector Institute</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>NeurIPS 2022</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="assets/main.pdf">
                <span class="material-icons"> description </span> 
                PDF 
            </a>
            <!--
            <a class="supp-btn" href="https://arxiv.org/abs/2207.01725">
                <span class="material-icons"> description </span> 
                ArXiv 
            </a>
            <a class="supp-btn" href="assets/CVPR_2022_poster.pdf">
                <span class="material-icons"> description </span> 
                  Poster
            </a>
            <a class="supp-btn" href="https://openaccess.thecvf.com/content/CVPR2022/html/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.html">
                <span class="material-icons">description </span>
                  DOI
            </a>
            -->
            <a class="supp-btn" href="assets/bib.txt">
                <span class="material-icons"> description </span> 
                  BibTeX
            </a>
        </div></div>
    </div>

    <section id="videos">
        <figure style="width: 75%">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/TwitterPreviewSlides.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
    </section>

    <!--
    <section id="teaser">
            <figure style="width: 90%;">
                <a href="assets/imagenet_regression_summary_v2.png">
                    <img width="55%" src="assets/imagenet_regression_summary_v2.png">
                </a>
                <a href="assets/Regression_Functions.PNG">
                    <img width="44%" src="assets/Regression_Functions.PNG" style="padding-bottom: 110px">
                </a>
                <p class="caption">
                    Extrapolating the learning curve <b>(Left Figure)</b> on ImageNet as a function
                    of data set size when given 10% of the data set (125, 000 images; dotted curves)
                    and 50% (600, 000 images; dashed curves) using four regression functions <b>(Right Table)</b>. 
                    The vertical lines show how much data is needed to
                    meet a desired 67% validation accuracy according to each dashed
                    curve. We observe:
                    <b>(1)</b> with a small initial set (i.e., 10%), all of the regression functions diverge
                    from the ground truth learning curve; 
                    <b>(2)</b> with enough data (i.e., 50%), the functions accurate extrapolate performance; but
                    <b>(3)</b> even a small extrapolation error can yield poor data estimates.
                    Although the functions have an error of 1-6% from the ground truth (67%
                    at 900, 000 images), they under/over-estimate the data requirement by
                    120, 000 to 310, 000 images.
                </p>
            </figure>
    </section>
    -->

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            Modern deep learning systems require huge data sets to achieve 
            impressive performance, but there is little guidance on how 
            much or what kind of data to collect. Over-collecting data 
            incurs unnecessary present costs, while under-collecting may 
            incur future costs and delay workflows. We propose a new paradigm 
            for modeling the data collection workflow as a formal <i>optimal 
            data collection problem</i> that allows designers to specify performance 
            targets, collection costs, a time horizon, and penalties for failing 
            to meet the targets. Additionally, this formulation generalizes to 
            tasks requiring multiple data sources, such as labeled and 
            unlabeled data used in semi-supervised learning. To solve our 
            problem, we develop Learn-Optimize-Collect (LOC), which minimizes 
            expected future collection costs. Finally, we numerically compare 
            our framework to the conventional baseline of estimating data 
            requirements by extrapolating from neural scaling laws. We 
            significantly reduce the risks of failing to meet desired 
            performance targets on several classification, segmentation, 
            and detection tasks, while maintaining low total collection costs.
        </p>
        <!--
        <h3>Guidelines for collecting just enough data to meet performance targets</h3>
        <ul class="insight">
            <li>Account for multiple (e.g., five) rounds of sequential data collection. 
                It is hard to accurately estimate how much data is needed in one shot.</li>
            <li>Use regression functions that tend to under-estimate the data requirement 
                in order to avoid over-collecting data by large margins.
            </li>
            <li>If the desired performance is <i>V*</i>, estimate the data requirement 
                for <i>V*+&#964;</i>. Use previous tasks & data sets to 
                learn a good value for this correction factor.</li>
        </ul>
        -->
    </section>

    <section id="problem">
    <h2>Main Problem</h2>
    <hr>
        <p>
            <b>
            There is an industry folklore that
            <a href="https://venturebeat.com/ai/why-do-87-of-data-science-projects-never-make-it-into-production/">
                87% of AI systems do not make it to production.
            </a>
            Data collection is a fundamental challenge. For example, a 2019 survey revealed
            <a href="https://content.alegion.com/dimensional-researchs-survey">
                51% of enterprise teams struggled to collect enough data for their models.
            </a>
            </b> 
        </p>

        <ul class="insight">
            <b>Example.</b>
            A startup is developing an object detector for use in autonomous vehicles within the next T=5 months. 
            Their model must achieve a mean Average Precision greater than V*=95% on a pre-determined validation 
            set or else they will lose an expected profit of P = 1,000,000 dollars. Collecting training data requires 
            employing drivers to record videos and annotators to label the data, where the marginal cost of 
            obtaining each image is approximately c = 1 dollar. In order to manage annual finances, the startup must plan 
            how much data to collect at the beginning of each year.
        </ul>
        <p>
            Given a target performance <i>V*</i>, initial set of <i>q<sub>0</sub></i> points, 
            cost-per-sample <i>c</i>, maximum number of collection rounds <i>T</i>, and a 
            penalty <i>P</i> for failing to meet the target, we must determine how much data
            to collect <i>q<sub>t</sub></i> in each round to minimize the total costs and penalties.
            In each round, we collect more data and re-evaluate our model until we reach the target or
            the final round.
        </p>
        <!--
        <figure style="width: 95%;">
            <a href="assets/imagenet_regression_summary_v2.png">
                <img width="55%" src="assets/imagenet_regression_summary_v2.png">
            </a>
            <a href="assets/Regression_Functions.PNG">
                <img width="44%" src="assets/Regression_Functions.PNG" style="padding-bottom: 110px">
            </a>
        </figure>
        -->
    </section>

    <section id="loc">
    <h2>Learn-Optimize-Collect</h2>
    <hr>
        <figure style="width: 90%;">
            <a href="assets/FlowChart.PNG">
                <img width="100%" src="assets/FlowChart.PNG">
            </a>
        </figure>
        <p>
            In each round, we first estimate the probability distribution of how much data we need
            by bootstrap resampling different scaling laws and fitting a Density Estimation model.
            We then solve a differentiable optimization problem to minimize the likelihood of not collecting
            enough data (obtained from the DE model) plus the total collection cost. 
            <!-- This objective is differentiable.
            -->
        </p>
    <h3>1) Learning the data requirement distribution</h3>
        <figure style="width: 90%;">
            <a href="assets/LearnAnimation.gif">
                <img width="100%" src="assets/LearnAnimation.gif">
            </a>
        </figure>
    <h3>2) Optimizing how much data to collect</h3>
        <figure style="width: 90%;">
            <a href="assets/OptimizeAnimation.gif">
                <img width="100%" src="assets/OptimizeAnimation.gif">
            </a>
        </figure>

    </section>

    <section id="extension">
    <h2>Extensions to Multiple Data Sources</h2>
    <hr>
        <p>
            The optimization framework naturally generalizes to more complex settings such as when
            we have multiple types of data arriving from different sources. Consider:
            <li>Semi-supervised learning: we can train with labeled and unlabeled data sets, where collecting labeled data incurs an additional cost over unlabeled.</li>
            <li>Long-tail learning: some classes may be hard-to-collect and therefore, more expensive, than other classes.</li>
            <li>Domain adaptation and synthetic-to-real: source (synthetic) training data can be collected more easily than target (real) data.</li> 

            In each case, we have different categories of data that incur different costs-per-sample of collection. Although we need data of each category, we
            may achieve our performance targets even with collecting more of the cheaper data and less of the costly data. <b>The learning and optimization problems
            can be adapted to this setting.
            </b>
        </p>
        <figure style="width: 90%;">
            <a href="assets/ExtensionAnimation.gif">
                <img width="100%" src="assets/ExtensionAnimation.gif">
            </a>
        </figure>

    </section>



    <section id="Results">
        <h2>Results</h2>
    <hr>
        <p>
            For each data set and task, we fix the number of rounds <i>T</i> and then sweep the performance target
            <i>V*</i> to see how well a policy makes decisions on how much data to collect. We compare LOC against 
            the intuitive baseline of using a single neural scaling law to estimate how much data is needed.
            We evaluate on two metrics:
            <li>Failure rate: For each data set, task, and <i>T</i>, how often does a policy fail to collect enough
            data to meet the target.</li>
            <li>Cost ratio: For each data set, task, and <i>T</i>, what is the average ratio of the cost incurred 
            <i>c(q<sub>T</sub> - q<sub>0</sub>)</i> over the minimum possible cost <i>c(q* - q<sub>0</sub>)</i> where
            <i>q*</i> is the absolute minimum amount of data needed to reach the performance target. In other words,
            this measures the relative sub-optimality of a policy with respect to the optimization problem. </li>
        </p>
    
        <ul class="insight">
            <b>
            LOC reduces the chances of failing to collect enough data to less than 5% whenever we have more than 1 
            collection round. Moreover, we almost always spend at most 2 times the minimum possible cost. In 
            comparison, regression methods fail more than 50% of the time for almost all data sets and tasks.
            </b>
        </ul>
        <figure style="width: 90%;">
            <a href="assets/Table1.PNG">
                <img class="center" width="70%" src="assets/Table1.PNG">
            </a>
        </figure>

        <p>
            We also consider two new problems with multiple data types. First, we consider classification on 
            CIFAR-100 where the first 50 classes cost more than the second 50 classes (e.g., as in long-tail
            learning). Second, we consider segmentation on BDD100K where we can pseudo-label data from an
            additional unlabeled source to augment training.
        </p>
        
        <ul class="insight">
            <b>
            LOC consistently reduces the chances of failing while maintaining low costs for almost all settings.
            </b>
        </ul>
        <figure style="width: 90%;">
            <a href="assets/Table2.PNG">
                <img class="center" width="70%" src="assets/Table2.PNG">
            </a>
        </figure>
    
    
    </section>
    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>
            @InProceedings{Mahmood_2022_Optimizing,
                author    = {Mahmood, Rafid and Lucas, James and Alvarez, Jose M. and Fidler, Sanja 
                                and Law, Marc T.},
                title     = {Optimizing Data Collection for Machine Learning},
                booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
                month     = {November},
                year      = {2022}
        </code></pre>
    </section>

    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <figure style="width: 20%;">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.html">
                <img width="100%" src="assets/Paper_FrontPage.PNG">
            </a>
        </figure>
        <hr>
    </section>

<!--
<script type="module">
    import * as THREE from "https://unpkg.com/three@0.127.0/build/three.module.js";
    import {OrbitControls} from "https://unpkg.com/three@0.127.0/examples/jsm/controls/OrbitControls.js";
    import {OBJLoader} from "https://unpkg.com/three@0.127.0/examples/jsm/loaders/OBJLoader.js";

    // Render the predictions
    function random_choice(arr, n) {
        var index_set = {};
        var choice = [];
        while (choice.length < n) {
            var idx = Math.floor(Math.random() * arr.length);
            if (index_set[idx] !== undefined) {
                continue;
            }
            index_set[idx] = 0;
            choice.push(idx);
        }

        return choice.map(x => arr[x]);
    }

    function progress_bar() {
        var el = document.createElement("div");
        el.classList.add("progress");

        return {
            domElement: el,
            update: function (percent) {
                percent = Math.min(1, Math.max(0, percent));
                el.style.display = "block";
                el.style.width = Math.round(percent * 100) + "%";
            },
            hide: function () {
                el.style.display = "none";
            }
        };
    }

    function reset_checkboxes(checkboxes) {
        Array.prototype.forEach.call(checkboxes, function (c) {
            c.checked = false;
        });
        checkboxes[0].checked = true;
        checkboxes[1].checked = true;
    }

    function show_object(el, prefix, N) {
        const scene = new THREE.Scene();
        const renderer = new THREE.WebGLRenderer();
        const camera = new THREE.PerspectiveCamera(75, 1, 0.1, 1000);
        const controls = new OrbitControls(camera, renderer.domElement);

        camera.position.set(0.55, 0.55, 0.55);
        controls.target.set(0, 0, 0);
        controls.autoRotate = true;
        controls.autoRotateSpeed = 4;
        scene.background = new THREE.Color("white");
        var size = el.dataset.size;
        renderer.setSize(size, size);
        var progress = progress_bar();
        el.appendChild(progress.domElement);
        el.appendChild(renderer.domElement);

        const spotLight = new THREE.SpotLight( 0x909090 );
        spotLight.position.set( -100, -1000, -100 );

        spotLight.castShadow = true;
        scene.add( spotLight );

        const spotLight_2 = new THREE.SpotLight( 0x909090 );
        spotLight_2.position.set(100, 1000, 100 );

        spotLight_2.castShadow = true;
        scene.add( spotLight_2 );

        const spotLight_3= new THREE.SpotLight( 0x909090 );
        spotLight_3.position.set(0, 0, 100000 );

        spotLight_3.castShadow = true;
        scene.add( spotLight_3);

        const spotLight_4= new THREE.SpotLight( 0x909090 );
        spotLight_4.position.set(0, 0, -100000 );

        spotLight_4.castShadow = true;
        scene.add( spotLight_4);

        const colors = [
            0xf4f4f4,
            0xbc96dc,
        ];

        var previous_canvas_size = size;
        function animate() {
            requestAnimationFrame(animate);
            if (el.offsetWidth != previous_canvas_size) {
                previous_canvas_size = el.offsetWidth;
                renderer.domElement.style.width = previous_canvas_size + "px";
                renderer.domElement.style.height = previous_canvas_size + "px";
            }

            controls.update();
            renderer.render(scene, camera);
        }

        const loader = new OBJLoader();
        var meshes = [];
        var progresses = [];
        var loaded = 0;
        function load_part(part_idx) {
            progresses[part_idx] = 0;
            loader.load(
                prefix + "/part_00" + i + ".obj",
                function (object) {
                    var g = object.children[0].geometry;
                    var m = new THREE.MeshLambertMaterial({color: colors[part_idx]});
                    m.side = THREE.DoubleSide;
                    //g.computeVertexNormals();
                    var mesh = new THREE.Mesh(g, m);
                    meshes[part_idx] = mesh;

                    scene.add(mesh);

                    loaded++;
                    if (loaded == N) {
                        progress.hide();
                    }
                },
                function (event) {
                    progresses[part_idx] = event.loaded / event.total;
                    var total_progress = 0;
                    for (var i=0; i<progresses.length; i++) {
                        total_progress += progresses[i] / progresses.length;
                    }
                    progress.update(total_progress);
                }
            )
        }
        for (var i=0; i<N; i++) {
            load_part(i);
        }
        animate();

        return {
            meshes: meshes,
            show: function (indices) {
                for (var i=0; i<N; i++) {
                    //meshes[i].material.opacity = 0.0;
                    meshes[i].visible = false;
                }
                for (var i=0; i<indices.length; i++) {
                    //meshes[indices[i]].material.opacity = 1;
                    meshes[indices[i]].visible = true;
                }
            },
            show_all: function () {
                for (var i=0; i<N; i++) {
                    //meshes[i].material.opacity = 1;
                    meshes[i].visible = true;
                }
            },
            set_size: function(width, height) {
                renderer.setSize(width, height);
            }
        };
    }


    function show_group(elements, objects, N) {
        var controls = [];
        for (var i=0; i<elements.length; i++) {
            if (i==0) {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/conv_occnet/' + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            } else if (i==1) {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/ours/' + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            } else {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/ground_truth/'  + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            }
        }

        return {
            controls: controls,
            show: function (indices) {
                for (var i=0; i<controls.length; i++) {
                    controls[i].show(indices);
                }
            },
            show_all: function () {
                for (var i=0; i<controls.length; i++) {
                    controls[i].show_all();
                }
            }
        };
    }

    var shapenet = [
    "40",
    "1320",
    "1560",
    "1680",
    "2000",
    "2880",
    "3120",
    "3700",
    "3780",
    "4260",
    "4960",
    "5020",
    "5440",
    "6060",
    "6960",
    "7740",
    "8620",
    ];

    var shapenet_control = show_group(
        document.getElementById("shapenet").getElementsByClassName("render_window"),
        [shapenet[12], shapenet[1], shapenet[2]],
        2
    );
    var shapenet_checkboxes = document.querySelectorAll("#shapenet .controls input");
    reset_checkboxes(shapenet_checkboxes);
    document.querySelector("#shapenet .controls").addEventListener(
        "change",
        function (ev) {

            var ids = new Set();
            var part_ids = [0, 1];
            for (var i=0; i<shapenet_checkboxes.length; i++) {
                if (shapenet_checkboxes[i].checked) {
                    ids.add(part_ids[i]);
                }
            }

            shapenet_control.show(Array.from(ids));
        }
    );
    document.querySelector("#shapenet .controls button").addEventListener(
        "click",
        function (ev) {
            reset_checkboxes(shapenet_checkboxes);
            var new_shapenet = random_choice(shapenet, 1);
            var render_windows = document.getElementById("shapenet").getElementsByClassName("render_window");
            Array.prototype.forEach.call(render_windows, function (r) {r.innerHTML = "";});
            shapenet_control = show_group(
                render_windows,
                new_shapenet,
                2
            );
        }
    );



</script>
-->
</body>
</html>